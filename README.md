# Super README | Aula PrÃ¡tica de Apache Airflow com Docker Compose e Astro CLI

Este projeto foi desenvolvido como material de apoio para a aula **"Apache Airflow | Aula PrÃ¡tica"** do programa Lighthouse. O objetivo Ã© ensinar, na prÃ¡tica, como estruturar, executar, configurar e testar DAGs no Apache Airflow utilizando Docker Compose e, posteriormente, o Astro CLI.

> **Importante:** A aula parte do pressuposto de que os alunos jÃ¡ tiveram uma introduÃ§Ã£o teÃ³rica ao Airflow e Ã  orquestraÃ§Ã£o de dados. Por isso, aqui focamos 100% na prÃ¡tica.

---

## âœ¨ VisÃ£o Geral

Este repositÃ³rio estÃ¡ organizado com base nas boas prÃ¡ticas de engenharia de dados e ensino. As DAGs foram desenvolvidas de forma incremental, com o objetivo de evoluir o aprendizado a cada exemplo. Cada DAG tem um propÃ³sito didÃ¡tico especÃ­fico.

## ğŸ“š Ferramentas e Tecnologias Utilizadas

| Ferramenta         | FunÃ§Ã£o                                                           |
| ------------------ | ---------------------------------------------------------------- |
| **Apache Airflow** | OrquestraÃ§Ã£o de workflows de dados.                              |
| **Docker Compose** | Ambiente padronizado para execuÃ§Ã£o do Airflow.                   |
| **Makefile**       | Automatiza comandos do Docker Compose para facilitar a execuÃ§Ã£o. |
| **Poetry**         | Gerenciamento de dependÃªncias do projeto Python.                 |
| **pipx**           | Instala ferramentas CLI Python de forma isolada.                 |
| **pyenv**          | Gerencia mÃºltiplas versÃµes do Python em paralelo.                |
| **Astro CLI**      | Interface da Astronomer para executar e testar DAGs localmente.  |

### âš ï¸ ObservaÃ§Ã£o
As ferramentas para genrenciamento de versÃµes e controle de ambientes virtuais (pyenv, pipx e poetry) **NÃƒO** sÃ£o obrigÃ¡tÃ³rias para reproduÃ§Ã£o do projeto. VocÃªs podem estar usando a que for mais confortÃ¡vel para vocÃªs.
---

## ğŸŒ Requisitos de Ambiente

Para reproduzir este projeto, siga esta ordem de instalaÃ§Ã£o e configuraÃ§Ã£o no seu ambiente local (de preferÃªncia via WSL no Windows ou Linux/macOS). Mais instruÃ§Ãµes vÃ£o estar listada posteriormente neste documento:

### 1. InstalaÃ§Ã£o do `pyenv`

Permite alternar entre mÃºltiplas versÃµes do Python. Instalamos, por exemplo, o Python 3.11.8 com:

```bash
curl https://pyenv.run | bash
# Adicione as configuraÃ§Ãµes ao seu shell (bash/zsh)
# Reinicie o terminal apÃ³s isso
pyenv install 3.11.8
pyenv global 3.11.8
```

### 2. InstalaÃ§Ã£o do `pipx`

Facilita a instalaÃ§Ã£o de CLIs Python sem poluir o ambiente global:

```bash
python -m pip install --user pipx
pipx ensurepath
```

Feche e reabra o terminal ou execute `source ~/.bashrc` ou `source ~/.zshrc`.

### 3. InstalaÃ§Ã£o do `poetry`

Gerencia o ambiente virtual e dependÃªncias do projeto:

```bash
pipx install poetry
```

Configure o poetry para criar os ambientes virtuais dentro do diretÃ³rio do projeto:

```bash
poetry config virtualenvs.in-project true
```

### 4. Instalar o Astro CLI

Usado para testar DAGs localmente em ambientes compatÃ­veis com o Astronomer:

```bash
curl -sSL https://install.astronomer.io | sudo sh
```

Verifique a instalaÃ§Ã£o com:

```bash
astro version
```

---

## ğŸš€ Iniciando o Projeto com Poetry

### âœ… Passo 1 â€“ Criar o diretÃ³rio do projeto

```bash
mkdir airflow-lighthouse-aula-pratica
cd airflow-lighthouse-aula-pratica
```

### âœ… Passo 2 â€“ Inicializar o projeto com o Poetry

```bash
poetry init
```

Siga as instruÃ§Ãµes do terminal e escolha `n` para todas as dependÃªncias por enquanto (vamos instalar depois manualmente).

### âœ… Passo 3 â€“ Ativar a versÃ£o do Python desejada com o pyenv

Certifique-se de estar usando o Python correto:

```bash
pyenv local 3.11.8
```

Esse comando cria um arquivo `.python-version` no projeto. Isso garante que o Poetry vai usar a versÃ£o correta ao criar o ambiente virtual.

### âœ… Passo 4 â€“ Criar o ambiente virtual

```bash
poetry install
```

O Poetry vai criar o ambiente virtual em `.venv/` e instalar as dependÃªncias do `pyproject.toml` (inicialmente vazio, mas serÃ¡ preenchido ao longo do projeto).

> ğŸ’¡ **Dica:** Sempre ative o ambiente virtual antes de rodar qualquer coisa:

```bash
poetry shell
```

### âœ… Passo 5 â€“ Instalar dependÃªncias principais do projeto

```bash
poetry add apache-airflow requests
```

---

## ğŸš§ Estrutura do Projeto (Docker Compose)

```
.
â”œâ”€â”€ .git/ # DiretÃ³rio de versionamento Git
â”œâ”€â”€ .venv/ # Ambiente virtual Python
â”œâ”€â”€ dags/ # ContÃ©m todas as definiÃ§Ãµes das DAGs do Airflow
â”‚ â”œâ”€â”€ pycache/ # Cache de mÃ³dulos Python
â”‚ â”œâ”€â”€ utils/ # MÃ³dulos de utilidades e configuraÃ§Ãµes para as DAGs
â”‚ â”‚ â”œâ”€â”€ pycache/
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ config.py # ConfiguraÃ§Ãµes gerais
â”‚ â”‚ â”œâ”€â”€ env.py # VariÃ¡veis de ambiente
â”‚ â”‚ â””â”€â”€ messages.py # Mensagens ou textos para as DAGs
â”‚ â”œâ”€â”€ 1-dag_variables_operator.py # Exemplo de DAG utilizando Airflow Variables via operador
â”‚ â”œâ”€â”€ 2-dag_variables_via_ui.py # Exemplo de DAG obtendo variÃ¡veis via UI
â”‚ â”œâ”€â”€ 3-dag_variables_from_env.py # Exemplo de DAG lendo variÃ¡veis de ambiente
â”‚ â”œâ”€â”€ 4-dag_config_example.py # Exemplo de DAG com configuraÃ§Ã£o externa
â”‚ â”œâ”€â”€ 5-dag_pipeline_simples.py # DAG que demonstra um pipeline de dados simples
â”‚ â”œâ”€â”€ 6-dag_docker_operator_example.py # DAG usando DockerOperator
â”‚ â””â”€â”€ 7-dag_taskflow_xcom.py # DAG com TaskFlow API e XComs
â”œâ”€â”€ infra/ # DefiniÃ§Ãµes de infraestrutura (Docker Compose)
â”‚ â””â”€â”€ docker-compose.yml # Arquivo para orquestrar serviÃ§os Docker
â”œâ”€â”€ logs/ # DiretÃ³rio para logs de execuÃ§Ã£o do Airflow e DAGs
â”‚ â”œâ”€â”€ dag_id=dag_docker_operator_example/
â”‚ â”œâ”€â”€ dag_id=dag_pipeline_simples/
â”‚ â”œâ”€â”€ dag_processor_manager/
â”‚ â””â”€â”€ scheduler/
â”œâ”€â”€ output/ # DiretÃ³rio para dados de saÃ­da gerados pelas DAGs
â”‚ â””â”€â”€ estados_ibge.csv # Exemplo de arquivo de saÃ­da
â”œâ”€â”€ .env # Arquivo para variÃ¡veis de ambiente locais
â”œâ”€â”€ .gitignore # Regras para ignorar arquivos no Git
â”œâ”€â”€ .python-version # Define a versÃ£o do Python (usado pelo pyenv)
â”œâ”€â”€ logs.sh # Script para visualizar logs dos contÃªineres
â”œâ”€â”€ Makefile # Orquestrador de comandos de build e execuÃ§Ã£o
â”œâ”€â”€ poetry.lock # Gerenciamento de dependÃªncias com Poetry
â”œâ”€â”€ pyproject.toml # ConfiguraÃ§Ã£o do projeto e dependÃªncias com Poetry
â”œâ”€â”€ README.md # Este arquivo de documentaÃ§Ã£o
â”œâ”€â”€ reset.sh # Script para resetar o ambiente Airflow
â”œâ”€â”€ start.sh # Script para iniciar o ambiente Airflow
â””â”€â”€ stop.sh # Script para parar o ambiente Airflow

```
### âš ï¸ ObservaÃ§Ã£o
As pastas logs e output nÃ£o vÃ£o estar no repositÃ³rio pois estÃ£o no gitignore.
---
## ğŸ§© DAG 1: 1_dag_variables_operator.py

### ğŸ¯ Objetivo

Introduzir o conceito de DAG no Airflow e mostrar como utilizar o operador PythonOperator com funÃ§Ãµes Python simples.

### ğŸ” O que essa DAG faz?

Ela lÃª uma variÃ¡vel de ambiente (via Airflow Variables) e imprime o conteÃºdo usando uma task com PythonOperator. Foi feita para apresentar:

* O uso do @dag e @task
* A estrutura mÃ­nima de uma DAG
* Como fazer um log no Airflow com context['ti'].xcom_push
* Como recuperar variÃ¡veis da interface do Airflow

### ğŸ’¡ Conceitos abordados

* CriaÃ§Ã£o bÃ¡sica de DAG
* Uso de PythonOperator
* Airflow Variables
* Logging no Airflow

### âœ… Boas prÃ¡ticas utilizadas

* Uso de logs estruturados
* SeparaÃ§Ã£o clara entre definiÃ§Ã£o da DAG e lÃ³gica de negÃ³cio
* NomeaÃ§Ã£o intuitiva da DAG e das tasks

---
## ğŸ§© DAG 2: `2_dag_variables_via_ui.py`

### ğŸŒŸ Objetivo

Demonstrar como utilizar as **Airflow Variables** de maneira mais prÃ¡tica, definindo seus valores diretamente pela interface Web do Airflow (Web UI), em vez de hardcoded no cÃ³digo Python.

Essa DAG tem o propÃ³sito de mostrar para os alunos uma maneira mais flexÃ­vel e recomendada de parametrizar o comportamento de DAGs em produÃ§Ã£o.

### ğŸ” O que essa DAG faz?

Essa DAG lÃª o valor de uma variÃ¡vel chamada `mensagem_ui` definida diretamente no menu "Admin > Variables" da interface do Airflow, e imprime seu conteÃºdo no log. Para isso, ela utiliza o `PythonOperator` chamando uma funÃ§Ã£o que acessa a variÃ¡vel com `Variable.get()`.

### ğŸ“Š Conceitos abordados

* UtilizaÃ§Ã£o de Airflow Variables via Web UI
* Como recuperar valores de variÃ¡veis usando `Variable.get()`
* Separar a parametrizaÃ§Ã£o da lÃ³gica de execuÃ§Ã£o
* Boas prÃ¡ticas para tornar o cÃ³digo mais reutilizÃ¡vel e amigÃ¡vel para alteraÃ§Ãµes futuras

### âœ… Boas prÃ¡ticas utilizadas

* Uso de variÃ¡veis externas para configurar comportamento
* A DAG falha com erro explicativo caso a variÃ¡vel nÃ£o exista, incentivando validaÃ§Ã£o
* ComentÃ¡rios explicativos no cÃ³digo
* CriaÃ§Ã£o de uma Ãºnica task simples com foco em clareza

### âš ï¸ ObservaÃ§Ã£o

Essa DAG sÃ³ funcionarÃ¡ corretamente caso a variÃ¡vel `mensagem_ui` tenha sido previamente criada via UI do Airflow. Caso contrÃ¡rio, uma exceÃ§Ã£o serÃ¡ levantada durante a execuÃ§Ã£o da task. Isso reforÃ§a a importÃ¢ncia da configuraÃ§Ã£o correta do ambiente antes de rodar a DAG.

---
## ğŸ§© DAG 3: `3_dag_variables_from_env.py`

### ğŸ¯ Objetivo

Mostrar como tornar as DAGs mais portÃ¡veis e seguras ao ler variÃ¡veis diretamente de um arquivo `.env`, sem depender da configuraÃ§Ã£o manual na interface do Airflow. Isso permite controle por versionamento e facilita a reproduÃ§Ã£o em diferentes ambientes.

### ğŸ” O que essa DAG faz?

Essa DAG Ã© uma evoluÃ§Ã£o da anterior. Ela:

* Usa o pacote `python-dotenv` para ler variÃ¡veis de ambiente de um arquivo `.env` localizado na raiz do projeto;
* Recupera essas variÃ¡veis diretamente no cÃ³digo Python usando `os.getenv()`;
* Faz logs das variÃ¡veis lidas.

### ğŸ“‚ Arquivos criados/modificados

* `.env`: Criado na raiz do projeto contendo as variÃ¡veis necessÃ¡rias.
* `3_dag_variables_from_env.py`: ContÃ©m a DAG que consome essas variÃ¡veis.

Exemplo do `.env`:

```dotenv
NOME=Lucas
IDADE=30
```

### âœ… DependÃªncia adicionada ao projeto

Adicionamos ao projeto o pacote `python-dotenv` via Poetry:

```bash
poetry add python-dotenv
```

### ğŸ’¡ Conceitos abordados

* Isolamento de configuraÃ§Ã£o por ambiente
* Uso de `.env` em projetos Python
* Uso de `os.getenv()`

### âœ… Boas prÃ¡ticas utilizadas

* **SeguranÃ§a**: evita salvar informaÃ§Ãµes sensÃ­veis diretamente no cÃ³digo.
* **Portabilidade**: facilita a execuÃ§Ã£o da DAG em qualquer ambiente com o mesmo `.env`.
* **OrganizaÃ§Ã£o**: separa variÃ¡veis de execuÃ§Ã£o do cÃ³digo-fonte.

---
## ğŸ§© DAG 4: `4_dag_config_example.py`

### ğŸ¯ Objetivo

Demonstrar como centralizar parÃ¢metros e configuraÃ§Ãµes da DAG em um mÃ³dulo separado (`config.py`). Isso promove organizaÃ§Ã£o e reutilizaÃ§Ã£o, alÃ©m de facilitar a manutenÃ§Ã£o.

### ğŸ” O que essa DAG faz?

Essa DAG Ã© uma evoluÃ§Ã£o da DAG anterior. Ela utiliza um arquivo `config.py` dentro da pasta `utils/` para centralizar informaÃ§Ãµes como:

* ID da DAG
* DescriÃ§Ã£o
* Data de inÃ­cio (start\_date)
* Intervalo de agendamento (schedule\_interval)

A DAG em si Ã© simples e tem apenas uma task que imprime uma mensagem, mas seu foco estÃ¡ na melhoria de arquitetura.

### ğŸ§± Componentes do Projeto Utilizados

* `dags/4_dag_config_example.py`: a DAG principal, agora com importaÃ§Ãµes vindas de `utils/config.py`.
* `dags/utils/config.py`: arquivo com variÃ¡veis de configuraÃ§Ã£o da DAG.

### âœ… Boas prÃ¡ticas utilizadas

* **CentralizaÃ§Ã£o de configuraÃ§Ãµes**: evita hardcoded e facilita alteraÃ§Ãµes futuras.
* **SeparaÃ§Ã£o de responsabilidades**: o arquivo `config.py` contÃ©m apenas configuraÃ§Ãµes, enquanto a DAG contÃ©m a lÃ³gica.
* **ReutilizaÃ§Ã£o de parÃ¢metros**: os valores definidos no `config.py` podem ser usados em mÃºltiplas DAGs.

### ğŸ“ Estrutura dos arquivos relacionados

```
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ 4_dag_config_example.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ config.py
```

### ğŸ’¡ Conceitos abordados

* OrganizaÃ§Ã£o de projetos Airflow
* SeparaÃ§Ã£o entre configuraÃ§Ã£o e cÃ³digo
* EvoluÃ§Ã£o incremental de DAGs

### ğŸ“Œ ObservaÃ§Ã£o

Essa DAG nÃ£o apresenta funcionalidades novas em termos de execuÃ§Ã£o ou operadores, mas Ã© um excelente exemplo de melhoria arquitetural. Essa refatoraÃ§Ã£o serÃ¡ Ãºtil para DAGs maiores e mais complexas ao longo do projeto.

## ğŸ§© DAG 5: `5_dag_pipeline_simples.py`

### ğŸ¯ Objetivo

Demonstrar uma pipeline simples de dados orquestrada pelo Airflow, com mÃºltiplas tarefas encadeadas. Essa DAG busca ilustrar um fluxo realista de ETL (Extract, Transform, Load), utilizando operadores bÃ¡sicos e boas prÃ¡ticas.

### ğŸ” O que essa DAG faz?

Essa DAG realiza o seguinte fluxo de trabalho:

1. **ExtraÃ§Ã£o de dados da API do IBGE**: Faz uma requisiÃ§Ã£o HTTP e salva os dados de estados brasileiros em CSV.
2. **TransformaÃ§Ã£o dos dados**: LÃª o CSV, transforma o conteÃºdo (por exemplo, filtrando ou formatando os dados).
3. **Carregamento**: Simula o carregamento dos dados transformados em algum destino (ex: banco ou storage).

Todos esses passos foram implementados com `PythonOperator`, de forma simples e didÃ¡tica.

### ğŸ’¡ Conceitos abordados

* CriaÃ§Ã£o de mÃºltiplas `PythonOperator` encadeadas
* Logging em cada etapa do pipeline
* ManipulaÃ§Ã£o de arquivos CSV em Python
* Encadeamento de tarefas com `.set_downstream()` ou via decorator (dependendo da versÃ£o)

### âœ… Boas prÃ¡ticas utilizadas

* SeparaÃ§Ã£o clara das responsabilidades de cada etapa do pipeline
* UtilizaÃ§Ã£o de logs para rastreabilidade
* NomeaÃ§Ã£o descritiva para DAGs e tasks
* CÃ³digo comentado para fins didÃ¡ticos

### ğŸ› ï¸ Outras observaÃ§Ãµes

* O arquivo CSV Ã© salvo no diretÃ³rio `output/`, que foi montado via volume no Docker.
* O cÃ³digo utiliza a biblioteca `requests` para comunicaÃ§Ã£o com a API pÃºblica do IBGE.
* Essa DAG Ã© um Ã³timo ponto de partida para mostrar como uma DAG pode representar um pipeline real e ser facilmente expandida.

---
## ğŸ§© DAG 6: `6_dag_docker_operator_example.py`

### ğŸŒŸ Objetivo

Apresentar como o Apache Airflow pode executar tarefas dentro de containers Docker utilizando o `DockerOperator`. Este exemplo Ã© essencial para demonstrar como integrar workflows do Airflow com ambientes isolados e reproduzÃ­veis, que Ã© uma prÃ¡tica comum em pipelines de dados modernas.

### ğŸ” O que essa DAG faz?

Esta DAG executa um container Docker que roda um script Python simples, localizado na pasta `include/hello-world.py`. O script apenas imprime uma mensagem no terminal, simulando uma tarefa de processamento isolada.

### âš–ï¸ Por que usamos o `DockerOperator`?

* Para garantir isolamento de ambiente.
* Para facilitar a portabilidade e reprodutibilidade.
* Para executar cÃ³digos que dependem de bibliotecas especÃ­ficas sem interferir no ambiente principal do Airflow.

### ğŸ“ Etapas realizadas

1. Criei o script `include/hello-world.py` que contÃ©m um print simples:

   ```python
   print("Hello from Docker!")
   ```

2. Configurei o caminho do volume que serÃ¡ montado no container, permitindo que o Airflow acesse o script.

3. Utilizei o `DockerOperator` para executar o container com a imagem `python:3.11-slim`.

4. Montei o volume da pasta `include/` para dentro do container em `/app`, e executei:

   ```bash
   python /app/hello-world.py
   ```

### ğŸ’¡ Boas PrÃ¡ticas Adotadas

* Utiliza imagens leves (como `python:3.11-slim`).
* Define o volume com `Mount` de forma explÃ­cita, garantindo controle de acesso aos arquivos.
* Define `auto_remove=True` para que o container seja removido apÃ³s a execuÃ§Ã£o.
* Usa nomes descritivos para tasks e DAG.

### âš ï¸ Pontos de AtenÃ§Ã£o

* O Docker precisa estar funcionando corretamente no host (e acessÃ­vel pelo Airflow).
* Ã‰ necessÃ¡rio garantir que o path local montado esteja correto e com permissÃ£o adequada.
* Containers com `auto_remove=False` podem acumular lixo se nÃ£o forem limpos.

### ğŸŒ Relacionamento com o Mundo Real

* Esse tipo de DAG Ã© comum em pipelines que executam scripts legados, modelos de machine learning ou qualquer cÃ³digo que tenha dependÃªncias especÃ­ficas.
* TambÃ©m Ã© muito usada para rodar transformaÃ§Ãµes pesadas que precisam de ambientes dedicados.

---
## ğŸ§© DAG 7: `7_dag_taskflow_xcom.py`

### ğŸŒŸ Objetivo

Apresentar o uso do decorator `@task` do TaskFlow API do Airflow e demonstrar como as tasks podem se comunicar entre si usando XComs de forma implÃ­cita (sem necessidade de `xcom_push` e `xcom_pull`).

Essa Ã© uma das formas mais modernas e legÃ­veis de se construir DAGs em Airflow.

---

### ğŸ” O que essa DAG faz?

* ExtraÃ­ uma lista de nomes de estados do Brasil (dados simulados ou importados de forma simples);
* Transforma os nomes (por exemplo, colocando todos em maiÃºsculas);
* Salva os resultados em um arquivo `.csv` dentro da pasta `output/`.

---

### ğŸ”§ Ferramentas utilizadas

* `@dag` e `@task` (TaskFlow API)
* ManipulaÃ§Ã£o de listas em Python
* Escrita de arquivos `.csv`

---

### âœ… Boas prÃ¡ticas utilizadas

* NomeaÃ§Ã£o clara de tasks
* Uso da TaskFlow API para facilitar o compartilhamento de dados via retorno de funÃ§Ãµes
* ModularizaÃ§Ã£o das etapas: `extrair`, `transformar`, `salvar`
* OrganizaÃ§Ã£o da saÃ­da em uma pasta dedicada `output/`

---

### ğŸ“ Conceitos didÃ¡ticos ensinados

* O que Ã© o TaskFlow API
* Como usar `@task` e `@dag`
* Como ocorre a troca de dados entre tasks com retorno de funÃ§Ãµes (XCom implÃ­cito)
* BenefÃ­cios dessa abordagem sobre o uso explÃ­cito de XComs
* Escrita e leitura de arquivos com Python dentro de DAGs

---

### ğŸ”— ConexÃ£o com as aulas anteriores

Esta Ã© uma evoluÃ§Ã£o natural das primeiras DAGs, pois apresenta uma abordagem mais moderna e robusta, preparando o terreno para workflows mais complexos e reutilizÃ¡veis.

Ao usar o TaskFlow, a legibilidade aumenta e o risco de erros com XComs manuais diminui.

## ğŸ“ Infraestrutura e ExecuÃ§Ã£o do Projeto com Docker Compose

Esta seÃ§Ã£o detalha todos os arquivos auxiliares criados para facilitar a execuÃ§Ã£o e o gerenciamento do projeto. Inclui explicaÃ§Ãµes do `docker-compose.yml`, scripts `.sh`, `Makefile`, organizaÃ§Ã£o dos diretÃ³rios e como tudo isso interage para que os alunos possam reproduzir 100% do ambiente.

---

### ğŸ“‚ `infra/docker-compose.yml`

Este Ã© o principal arquivo que define a infraestrutura do ambiente de orquestraÃ§Ã£o. Ele:

* Usa a imagem oficial do Astronomer Runtime (baseada em Apache Airflow)
* Define os volumes para montar DAGs, logs, plugins, e a pasta de output
* Mapeia as portas para acesso via navegador (Webserver)

Trecho importante:

```yaml
dags:
  - ./dags:/usr/local/airflow/dags
```

Isso garante que qualquer DAG escrita localmente estarÃ¡ visÃ­vel dentro do container do Airflow.

Outros pontos relevantes:

* `output/` tambÃ©m Ã© montado, pois Ã© onde salvamos dados extraÃ­dos
* Os logs da execuÃ§Ã£o ficam em `logs/`
* Plugins customizados estariam em `plugins/`, mesmo que nÃ£o tenhamos usado aqui

---

### ğŸ”¹ Estrutura de diretÃ³rios

```bash
.
â”œâ”€â”€ dags/                  # ContÃ©m as DAGs e o mÃ³dulo utils/
â”‚   â”œâ”€â”€ utils/             # FunÃ§Ãµes auxiliares reutilizÃ¡veis (como leitura de variÃ¡veis e logs)
â”œâ”€â”€ logs/                  # Logs gerados pela execuÃ§Ã£o do Airflow
â”œâ”€â”€ output/                # Arquivos de saÃ­da gerados pelas DAGs
â”œâ”€â”€ infra/                 # ContÃ©m o docker-compose.yml
â”œâ”€â”€ start.sh, stop.sh, reset.sh, logs.sh  # Scripts auxiliares
â”œâ”€â”€ Makefile               # Alternativa centralizada para os scripts
```

---

## Scripts de Gerenciamento do Ambiente

Para simplificar a interaÃ§Ã£o com o ambiente Docker do Apache Airflow e otimizar as tarefas do dia a dia, este projeto utiliza uma sÃ©rie de scripts shell (`.sh`). Eles abstraem comandos complexos do Docker e do Airflow CLI, tornando o processo de inicializaÃ§Ã£o, parada e monitoramento muito mais intuitivo.

**Importante:** Antes de executar qualquer script `.sh` pela primeira vez, certifique-se de que ele tenha permissÃµes de execuÃ§Ã£o. VocÃª pode conceder essas permissÃµes usando o comando `chmod +x nome_do_script.sh` no terminal.

A seguir, a descriÃ§Ã£o e o modo de uso de cada script:

### 1. `start.sh` - Iniciando o Ambiente Airflow

*   **FunÃ§Ã£o:** Este script Ã© o ponto de entrada para levantar todo o seu ambiente Apache Airflow, incluindo os serviÃ§os de Webserver, Scheduler, Worker, banco de dados (Postgres) e Redis, todos dentro de contÃªineres Docker. Ele tambÃ©m garante que o banco de metadados do Airflow seja inicializado e um usuÃ¡rio administrador seja criado, se ainda nÃ£o existirem.
*   **Quando usar:** Sempre que vocÃª precisar iniciar o ambiente Airflow do zero ou reiniciÃ¡-lo apÃ³s uma parada.
*   **Como usar:**
    ```bash
    ./start.sh
    ```
*   **Detalhes:** Ele executa os comandos `docker compose up -d` (para iniciar os serviÃ§os em segundo plano) e `docker compose exec airflow-webserver airflow db upgrade`, `airflow users create`, entre outros, para configurar o Airflow.

### 2. `stop.sh` - Parando o Ambiente Airflow

*   **FunÃ§Ã£o:** Este script Ã© responsÃ¡vel por parar todos os serviÃ§os do Airflow que estÃ£o rodando em contÃªineres Docker. Ele desliga os contÃªineres de forma controlada.
*   **Quando usar:** Quando vocÃª terminar de trabalhar no projeto e quiser liberar os recursos do seu computador, ou antes de realizar alteraÃ§Ãµes profundas no ambiente.
*   **Como usar:**
    ```bash
    ./stop.sh
    ```
*   **Detalhes:** Ele executa o comando `docker compose down`, que para e remove os contÃªineres, mas mantÃ©m os volumes de dados para persistÃªncia.

### 3. `reset.sh` - Resetando o Ambiente Airflow (Com Cuidado!)

*   **FunÃ§Ã£o:** Este Ã© um script de "limpeza total". Ele nÃ£o apenas para os serviÃ§os do Airflow, mas tambÃ©m remove *todos* os volumes de dados associados ao projeto (incluindo o banco de dados do Airflow). Isso significa que vocÃª perderÃ¡ o histÃ³rico de execuÃ§Ã£o de DAGs, logs antigos e configuraÃ§Ãµes de usuÃ¡rios.
*   **Quando usar:** **Use com extrema cautela!** Ã‰ ideal para quando vocÃª quer comeÃ§ar completamente do zero, como se tivesse acabado de clonar o repositÃ³rio, ou para resolver problemas persistentes com o banco de dados do Airflow. Na maioria dos casos, `stop.sh` e `start.sh` sÃ£o suficientes.
*   **Como usar:**
    ```bash
    ./reset.sh
    ```
*   **Detalhes:** Ele executa `docker compose down -v --remove-orphans`, garantindo uma limpeza profunda.

### 4. `logs.sh` - Visualizando os Logs do Ambiente

*   **FunÃ§Ã£o:** Este script permite visualizar os logs de todos os serviÃ§os do Airflow rodando em contÃªineres Docker em tempo real. Ã‰ essencial para depuraÃ§Ã£o e monitoramento do que estÃ¡ acontecendo no seu ambiente.
*   **Quando usar:** Sempre que precisar diagnosticar um problema, verificar se os serviÃ§os estÃ£o iniciando corretamente ou acompanhar a execuÃ§Ã£o de tasks.
*   **Como usar:**
    ```bash
    ./logs.sh
    ```
*   **Detalhes:** Ele executa `docker compose logs -f`, mostrando a saÃ­da de log de todos os contÃªineres e acompanhando novas linhas (modo "follow"). Pressione `Ctrl+C` para sair da visualizaÃ§Ã£o de logs.

---

### ğŸš€ Outra OpÃ§Ã£o para ExecuÃ§Ã£o (Makefile)

1. Ative o ambiente virtual com `poetry shell`
2. Inicie os containers:

```bash
make start
# ou
bash start.sh
```

3. Acesse o Airflow em `http://localhost:8080` com:

* **Login:** airflow
* **Senha:** airflow

4. Execute a DAG desejada na interface ou aguarde o agendamento

5. Veja os arquivos gerados na pasta `output/`

6. Acompanhe os logs com:

```bash
./logs.sh
```

---