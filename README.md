# Super README | Aula Pr√°tica de Apache Airflow com Docker Compose e Astro CLI

Este projeto foi desenvolvido como material de apoio para a aula **"Apache Airflow | Aula Pr√°tica"** do programa Lighthouse. O objetivo √© ensinar, na pr√°tica, como estruturar, executar, configurar e testar DAGs no Apache Airflow utilizando Docker Compose e, posteriormente, o Astro CLI.

> **Importante:** A aula parte do pressuposto de que os alunos j√° tiveram uma introdu√ß√£o te√≥rica ao Airflow e √† orquestra√ß√£o de dados. Por isso, aqui focamos 100% na pr√°tica.

---

## ‚ú® Vis√£o Geral

Este reposit√≥rio est√° organizado com base nas boas pr√°ticas de engenharia de dados e ensino. As DAGs foram desenvolvidas de forma incremental, com o objetivo de evoluir o aprendizado a cada exemplo. Cada DAG tem um prop√≥sito did√°tico espec√≠fico.

## üìö Ferramentas e Tecnologias Utilizadas

| Ferramenta         | Fun√ß√£o                                                           |
| ------------------ | ---------------------------------------------------------------- |
| **Apache Airflow** | Orquestra√ß√£o de workflows de dados.                              |
| **Docker Compose** | Ambiente padronizado para execu√ß√£o do Airflow.                   |
| **Makefile**       | Automatiza comandos do Docker Compose para facilitar a execu√ß√£o. |
| **Poetry**         | Gerenciamento de depend√™ncias do projeto Python.                 |
| **pipx**           | Instala ferramentas CLI Python de forma isolada.                 |
| **pyenv**          | Gerencia m√∫ltiplas vers√µes do Python em paralelo.                |
| **Astro CLI**      | Interface da Astronomer para executar e testar DAGs localmente.  |

### ‚ö†Ô∏è Observa√ß√£o
As ferramentas para genrenciamento de vers√µes e controle de ambientes virtuais (pyenv, pipx e poetry) **N√ÉO** s√£o obrig√°t√≥rias para reprodu√ß√£o do projeto. Voc√™s podem estar usando a que for mais confort√°vel para voc√™s.
---

## üåê Requisitos de Ambiente

Para reproduzir este projeto, siga esta ordem de instala√ß√£o e configura√ß√£o no seu ambiente local (de prefer√™ncia via WSL no Windows ou Linux/macOS). Mais instru√ß√µes v√£o estar listada posteriormente neste documento:

### 1. Instala√ß√£o do `pyenv`

Permite alternar entre m√∫ltiplas vers√µes do Python. Instalamos, por exemplo, o Python 3.11.8 com:

```bash
curl https://pyenv.run | bash
# Adicione as configura√ß√µes ao seu shell (bash/zsh)
# Reinicie o terminal ap√≥s isso
pyenv install 3.11.8
pyenv global 3.11.8
```

### 2. Instala√ß√£o do `pipx`

Facilita a instala√ß√£o de CLIs Python sem poluir o ambiente global:

```bash
python -m pip install --user pipx
pipx ensurepath
```

Feche e reabra o terminal ou execute `source ~/.bashrc` ou `source ~/.zshrc`.

### 3. Instala√ß√£o do `poetry`

Gerencia o ambiente virtual e depend√™ncias do projeto:

```bash
pipx install poetry
```

Configure o poetry para criar os ambientes virtuais dentro do diret√≥rio do projeto:

```bash
poetry config virtualenvs.in-project true
```

### 4. Instalar o Astro CLI

Usado para testar DAGs localmente em ambientes compat√≠veis com o Astronomer:

```bash
curl -sSL https://install.astronomer.io | sudo sh
```

Verifique a instala√ß√£o com:

```bash
astro version
```

---

## üöÄ Iniciando o Projeto com Poetry

### ‚úÖ Passo 1 ‚Äì Criar o diret√≥rio do projeto

```bash
mkdir airflow-lighthouse-aula-pratica
cd airflow-lighthouse-aula-pratica
```

### ‚úÖ Passo 2 ‚Äì Inicializar o projeto com o Poetry

```bash
poetry init
```

Siga as instru√ß√µes do terminal e escolha `n` para todas as depend√™ncias por enquanto (vamos instalar depois manualmente).

### ‚úÖ Passo 3 ‚Äì Ativar a vers√£o do Python desejada com o pyenv

Certifique-se de estar usando o Python correto:

```bash
pyenv local 3.11.8
```

Esse comando cria um arquivo `.python-version` no projeto. Isso garante que o Poetry vai usar a vers√£o correta ao criar o ambiente virtual.

### ‚úÖ Passo 4 ‚Äì Criar o ambiente virtual

```bash
poetry install
```

O Poetry vai criar o ambiente virtual em `.venv/` e instalar as depend√™ncias do `pyproject.toml` (inicialmente vazio, mas ser√° preenchido ao longo do projeto).

> üí° **Dica:** Sempre ative o ambiente virtual antes de rodar qualquer coisa:

```bash
poetry shell
```

### ‚úÖ Passo 5 ‚Äì Instalar depend√™ncias principais do projeto

```bash
poetry add apache-airflow requests
```

---

## üöß Estrutura do Projeto (Docker Compose)

```
.
‚îú‚îÄ‚îÄ .git/ # Diret√≥rio de versionamento Git
‚îú‚îÄ‚îÄ .venv/ # Ambiente virtual Python
‚îú‚îÄ‚îÄ dags/ # Cont√©m todas as defini√ß√µes das DAGs do Airflow
‚îÇ ‚îú‚îÄ‚îÄ pycache/ # Cache de m√≥dulos Python
‚îÇ ‚îú‚îÄ‚îÄ utils/ # M√≥dulos de utilidades e configura√ß√µes para as DAGs
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ pycache/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ init.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ config.py # Configura√ß√µes gerais
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ env.py # Vari√°veis de ambiente
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ messages.py # Mensagens ou textos para as DAGs
‚îÇ ‚îú‚îÄ‚îÄ 1-dag_variables_operator.py # Exemplo de DAG utilizando Airflow Variables via operador
‚îÇ ‚îú‚îÄ‚îÄ 2-dag_variables_via_ui.py # Exemplo de DAG obtendo vari√°veis via UI
‚îÇ ‚îú‚îÄ‚îÄ 3-dag_variables_from_env.py # Exemplo de DAG lendo vari√°veis de ambiente
‚îÇ ‚îú‚îÄ‚îÄ 4-dag_config_example.py # Exemplo de DAG com configura√ß√£o externa
‚îÇ ‚îú‚îÄ‚îÄ 5-dag_pipeline_simples.py # DAG que demonstra um pipeline de dados simples
‚îÇ ‚îú‚îÄ‚îÄ 6-dag_docker_operator_example.py # DAG usando DockerOperator
‚îÇ ‚îî‚îÄ‚îÄ 7-dag_taskflow_xcom.py # DAG com TaskFlow API e XComs
‚îú‚îÄ‚îÄ infra/ # Defini√ß√µes de infraestrutura (Docker Compose)
‚îÇ ‚îî‚îÄ‚îÄ docker-compose.yml # Arquivo para orquestrar servi√ßos Docker
‚îú‚îÄ‚îÄ logs/ # Diret√≥rio para logs de execu√ß√£o do Airflow e DAGs
‚îÇ ‚îú‚îÄ‚îÄ dag_id=dag_docker_operator_example/
‚îÇ ‚îú‚îÄ‚îÄ dag_id=dag_pipeline_simples/
‚îÇ ‚îú‚îÄ‚îÄ dag_processor_manager/
‚îÇ ‚îî‚îÄ‚îÄ scheduler/
‚îú‚îÄ‚îÄ output/ # Diret√≥rio para dados de sa√≠da gerados pelas DAGs
‚îÇ ‚îî‚îÄ‚îÄ estados_ibge.csv # Exemplo de arquivo de sa√≠da
‚îú‚îÄ‚îÄ .env # Arquivo para vari√°veis de ambiente locais
‚îú‚îÄ‚îÄ .gitignore # Regras para ignorar arquivos no Git
‚îú‚îÄ‚îÄ .python-version # Define a vers√£o do Python (usado pelo pyenv)
‚îú‚îÄ‚îÄ logs.sh # Script para visualizar logs dos cont√™ineres
‚îú‚îÄ‚îÄ Makefile # Orquestrador de comandos de build e execu√ß√£o
‚îú‚îÄ‚îÄ poetry.lock # Gerenciamento de depend√™ncias com Poetry
‚îú‚îÄ‚îÄ pyproject.toml # Configura√ß√£o do projeto e depend√™ncias com Poetry
‚îú‚îÄ‚îÄ README.md # Este arquivo de documenta√ß√£o
‚îú‚îÄ‚îÄ reset.sh # Script para resetar o ambiente Airflow
‚îú‚îÄ‚îÄ start.sh # Script para iniciar o ambiente Airflow
‚îî‚îÄ‚îÄ stop.sh # Script para parar o ambiente Airflow

```
### ‚ö†Ô∏è Observa√ß√£o
As pastas logs e output n√£o v√£o estar no reposit√≥rio pois est√£o no gitignore.
---
## üß© DAG 1: 1_dag_variables_operator.py

### üéØ Objetivo

Introduzir o conceito de DAG no Airflow e mostrar como utilizar o operador PythonOperator com fun√ß√µes Python simples.

### üîç O que essa DAG faz?

Ela l√™ uma vari√°vel de ambiente (via Airflow Variables) e imprime o conte√∫do usando uma task com PythonOperator. Foi feita para apresentar:

* O uso do @dag e @task
* A estrutura m√≠nima de uma DAG
* Como fazer um log no Airflow com context['ti'].xcom_push
* Como recuperar vari√°veis da interface do Airflow

### üí° Conceitos abordados

* Cria√ß√£o b√°sica de DAG
* Uso de PythonOperator
* Airflow Variables
* Logging no Airflow

### ‚úÖ Boas pr√°ticas utilizadas

* Uso de logs estruturados
* Separa√ß√£o clara entre defini√ß√£o da DAG e l√≥gica de neg√≥cio
* Nomea√ß√£o intuitiva da DAG e das tasks

---
## üß© DAG 2: `2_dag_variables_via_ui.py`

### üåü Objetivo

Demonstrar como utilizar as **Airflow Variables** de maneira mais pr√°tica, definindo seus valores diretamente pela interface Web do Airflow (Web UI), em vez de hardcoded no c√≥digo Python.

Essa DAG tem o prop√≥sito de mostrar para os alunos uma maneira mais flex√≠vel e recomendada de parametrizar o comportamento de DAGs em produ√ß√£o.

### üîç O que essa DAG faz?

Essa DAG l√™ o valor de uma vari√°vel chamada `mensagem_ui` definida diretamente no menu "Admin > Variables" da interface do Airflow, e imprime seu conte√∫do no log. Para isso, ela utiliza o `PythonOperator` chamando uma fun√ß√£o que acessa a vari√°vel com `Variable.get()`.

### üìä Conceitos abordados

* Utiliza√ß√£o de Airflow Variables via Web UI
* Como recuperar valores de vari√°veis usando `Variable.get()`
* Separar a parametriza√ß√£o da l√≥gica de execu√ß√£o
* Boas pr√°ticas para tornar o c√≥digo mais reutiliz√°vel e amig√°vel para altera√ß√µes futuras

### ‚úÖ Boas pr√°ticas utilizadas

* Uso de vari√°veis externas para configurar comportamento
* A DAG falha com erro explicativo caso a vari√°vel n√£o exista, incentivando valida√ß√£o
* Coment√°rios explicativos no c√≥digo
* Cria√ß√£o de uma √∫nica task simples com foco em clareza

### ‚ö†Ô∏è Observa√ß√£o

Essa DAG s√≥ funcionar√° corretamente caso a vari√°vel `mensagem_ui` tenha sido previamente criada via UI do Airflow. Caso contr√°rio, uma exce√ß√£o ser√° levantada durante a execu√ß√£o da task. Isso refor√ßa a import√¢ncia da configura√ß√£o correta do ambiente antes de rodar a DAG.

---
## üß© DAG 3: `3_dag_variables_from_env.py`

### üéØ Objetivo

Mostrar como tornar as DAGs mais port√°veis e seguras ao ler vari√°veis diretamente de um arquivo `.env`, sem depender da configura√ß√£o manual na interface do Airflow. Isso permite controle por versionamento e facilita a reprodu√ß√£o em diferentes ambientes.

### üîç O que essa DAG faz?

Essa DAG √© uma evolu√ß√£o da anterior. Ela:

* Usa o pacote `python-dotenv` para ler vari√°veis de ambiente de um arquivo `.env` localizado na raiz do projeto;
* Recupera essas vari√°veis diretamente no c√≥digo Python usando `os.getenv()`;
* Faz logs das vari√°veis lidas.

### üìÇ Arquivos criados/modificados

* `.env`: Criado na raiz do projeto contendo as vari√°veis necess√°rias.
* `3_dag_variables_from_env.py`: Cont√©m a DAG que consome essas vari√°veis.

Exemplo do `.env`:

```dotenv
NOME=Lucas
IDADE=30
```

### ‚úÖ Depend√™ncia adicionada ao projeto

Adicionamos ao projeto o pacote `python-dotenv` via Poetry:

```bash
poetry add python-dotenv
```

### üí° Conceitos abordados

* Isolamento de configura√ß√£o por ambiente
* Uso de `.env` em projetos Python
* Uso de `os.getenv()`

### ‚úÖ Boas pr√°ticas utilizadas

* **Seguran√ßa**: evita salvar informa√ß√µes sens√≠veis diretamente no c√≥digo.
* **Portabilidade**: facilita a execu√ß√£o da DAG em qualquer ambiente com o mesmo `.env`.
* **Organiza√ß√£o**: separa vari√°veis de execu√ß√£o do c√≥digo-fonte.

---
## üß© DAG 4: `4_dag_config_example.py`

### üéØ Objetivo

Demonstrar como centralizar par√¢metros e configura√ß√µes da DAG em um m√≥dulo separado (`config.py`). Isso promove organiza√ß√£o e reutiliza√ß√£o, al√©m de facilitar a manuten√ß√£o.

### üîç O que essa DAG faz?

Essa DAG √© uma evolu√ß√£o da DAG anterior. Ela utiliza um arquivo `config.py` dentro da pasta `utils/` para centralizar informa√ß√µes como:

* ID da DAG
* Descri√ß√£o
* Data de in√≠cio (start\_date)
* Intervalo de agendamento (schedule\_interval)

A DAG em si √© simples e tem apenas uma task que imprime uma mensagem, mas seu foco est√° na melhoria de arquitetura.

### üß± Componentes do Projeto Utilizados

* `dags/4_dag_config_example.py`: a DAG principal, agora com importa√ß√µes vindas de `utils/config.py`.
* `dags/utils/config.py`: arquivo com vari√°veis de configura√ß√£o da DAG.

### ‚úÖ Boas pr√°ticas utilizadas

* **Centraliza√ß√£o de configura√ß√µes**: evita hardcoded e facilita altera√ß√µes futuras.
* **Separa√ß√£o de responsabilidades**: o arquivo `config.py` cont√©m apenas configura√ß√µes, enquanto a DAG cont√©m a l√≥gica.
* **Reutiliza√ß√£o de par√¢metros**: os valores definidos no `config.py` podem ser usados em m√∫ltiplas DAGs.

### üìÅ Estrutura dos arquivos relacionados

```
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ 4_dag_config_example.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ config.py
```

### üí° Conceitos abordados

* Organiza√ß√£o de projetos Airflow
* Separa√ß√£o entre configura√ß√£o e c√≥digo
* Evolu√ß√£o incremental de DAGs

### üìå Observa√ß√£o

Essa DAG n√£o apresenta funcionalidades novas em termos de execu√ß√£o ou operadores, mas √© um excelente exemplo de melhoria arquitetural. Essa refatora√ß√£o ser√° √∫til para DAGs maiores e mais complexas ao longo do projeto.

## üß© DAG 5: `5_dag_pipeline_simples.py`

### üéØ Objetivo

Demonstrar uma pipeline simples de dados orquestrada pelo Airflow, com m√∫ltiplas tarefas encadeadas. Essa DAG busca ilustrar um fluxo realista de ETL (Extract, Transform, Load), utilizando operadores b√°sicos e boas pr√°ticas.

### üîç O que essa DAG faz?

Essa DAG realiza o seguinte fluxo de trabalho:

1. **Extra√ß√£o de dados da API do IBGE**: Faz uma requisi√ß√£o HTTP e salva os dados de estados brasileiros em CSV.
2. **Transforma√ß√£o dos dados**: L√™ o CSV, transforma o conte√∫do (por exemplo, filtrando ou formatando os dados).
3. **Carregamento**: Simula o carregamento dos dados transformados em algum destino (ex: banco ou storage).

Todos esses passos foram implementados com `PythonOperator`, de forma simples e did√°tica.

### üí° Conceitos abordados

* Cria√ß√£o de m√∫ltiplas `PythonOperator` encadeadas
* Logging em cada etapa do pipeline
* Manipula√ß√£o de arquivos CSV em Python
* Encadeamento de tarefas com `.set_downstream()` ou via decorator (dependendo da vers√£o)

### ‚úÖ Boas pr√°ticas utilizadas

* Separa√ß√£o clara das responsabilidades de cada etapa do pipeline
* Utiliza√ß√£o de logs para rastreabilidade
* Nomea√ß√£o descritiva para DAGs e tasks
* C√≥digo comentado para fins did√°ticos

### üõ†Ô∏è Outras observa√ß√µes

* O arquivo CSV √© salvo no diret√≥rio `output/`, que foi montado via volume no Docker.
* O c√≥digo utiliza a biblioteca `requests` para comunica√ß√£o com a API p√∫blica do IBGE.
* Essa DAG √© um √≥timo ponto de partida para mostrar como uma DAG pode representar um pipeline real e ser facilmente expandida.

---
## üß© DAG 6: `6_dag_docker_operator_example.py`

### üåü Objetivo

Apresentar como o Apache Airflow pode executar tarefas dentro de containers Docker utilizando o `DockerOperator`. Este exemplo √© essencial para demonstrar como integrar workflows do Airflow com ambientes isolados e reproduz√≠veis, que √© uma pr√°tica comum em pipelines de dados modernas.

### üîç O que essa DAG faz?

Esta DAG executa um container Docker que roda um script Python simples, localizado na pasta `include/hello-world.py`. O script apenas imprime uma mensagem no terminal, simulando uma tarefa de processamento isolada.

### ‚öñÔ∏è Por que usamos o `DockerOperator`?

* Para garantir isolamento de ambiente.
* Para facilitar a portabilidade e reprodutibilidade.
* Para executar c√≥digos que dependem de bibliotecas espec√≠ficas sem interferir no ambiente principal do Airflow.

### üìù Etapas realizadas

1. Criei o script `include/hello-world.py` que cont√©m um print simples:

   ```python
   print("Hello from Docker!")
   ```

2. Configurei o caminho do volume que ser√° montado no container, permitindo que o Airflow acesse o script.

3. Utilizei o `DockerOperator` para executar o container com a imagem `python:3.11-slim`.

4. Montei o volume da pasta `include/` para dentro do container em `/app`, e executei:

   ```bash
   python /app/hello-world.py
   ```

### üí° Boas Pr√°ticas Adotadas

* Utiliza imagens leves (como `python:3.11-slim`).
* Define o volume com `Mount` de forma expl√≠cita, garantindo controle de acesso aos arquivos.
* Define `auto_remove=True` para que o container seja removido ap√≥s a execu√ß√£o.
* Usa nomes descritivos para tasks e DAG.

### ‚ö†Ô∏è Pontos de Aten√ß√£o

* O Docker precisa estar funcionando corretamente no host (e acess√≠vel pelo Airflow).
* √â necess√°rio garantir que o path local montado esteja correto e com permiss√£o adequada.
* Containers com `auto_remove=False` podem acumular lixo se n√£o forem limpos.

### üåê Relacionamento com o Mundo Real

* Esse tipo de DAG √© comum em pipelines que executam scripts legados, modelos de machine learning ou qualquer c√≥digo que tenha depend√™ncias espec√≠ficas.
* Tamb√©m √© muito usada para rodar transforma√ß√µes pesadas que precisam de ambientes dedicados.

---
## üß© DAG 7: `7_dag_taskflow_xcom.py`

### üåü Objetivo

Apresentar o uso do decorator `@task` do TaskFlow API do Airflow e demonstrar como as tasks podem se comunicar entre si usando XComs de forma impl√≠cita (sem necessidade de `xcom_push` e `xcom_pull`).

Essa √© uma das formas mais modernas e leg√≠veis de se construir DAGs em Airflow.

---

### üîç O que essa DAG faz?

* Extra√≠ uma lista de nomes de estados do Brasil (dados simulados ou importados de forma simples);
* Transforma os nomes (por exemplo, colocando todos em mai√∫sculas);
* Salva os resultados em um arquivo `.csv` dentro da pasta `output/`.

---

### üîß Ferramentas utilizadas

* `@dag` e `@task` (TaskFlow API)
* Manipula√ß√£o de listas em Python
* Escrita de arquivos `.csv`

---

### ‚úÖ Boas pr√°ticas utilizadas

* Nomea√ß√£o clara de tasks
* Uso da TaskFlow API para facilitar o compartilhamento de dados via retorno de fun√ß√µes
* Modulariza√ß√£o das etapas: `extrair`, `transformar`, `salvar`
* Organiza√ß√£o da sa√≠da em uma pasta dedicada `output/`

---

### üéì Conceitos did√°ticos ensinados

* O que √© o TaskFlow API
* Como usar `@task` e `@dag`
* Como ocorre a troca de dados entre tasks com retorno de fun√ß√µes (XCom impl√≠cito)
* Benef√≠cios dessa abordagem sobre o uso expl√≠cito de XComs
* Escrita e leitura de arquivos com Python dentro de DAGs

---

### üîó Conex√£o com as aulas anteriores

Esta √© uma evolu√ß√£o natural das primeiras DAGs, pois apresenta uma abordagem mais moderna e robusta, preparando o terreno para workflows mais complexos e reutiliz√°veis.

Ao usar o TaskFlow, a legibilidade aumenta e o risco de erros com XComs manuais diminui.

## üìÅ Infraestrutura e Execu√ß√£o do Projeto com Docker Compose

Esta se√ß√£o detalha todos os arquivos auxiliares criados para facilitar a execu√ß√£o e o gerenciamento do projeto. Inclui explica√ß√µes do `docker-compose.yml`, scripts `.sh`, `Makefile`, organiza√ß√£o dos diret√≥rios e como tudo isso interage para que os alunos possam reproduzir 100% do ambiente.

---

### üìÇ `infra/docker-compose.yml`

Este √© o principal arquivo que define a infraestrutura do ambiente de orquestra√ß√£o. Ele:

* Usa a imagem oficial do Astronomer Runtime (baseada em Apache Airflow)
* Define os volumes para montar DAGs, logs, plugins, e a pasta de output
* Mapeia as portas para acesso via navegador (Webserver)

Trecho importante:

```yaml
dags:
  - ./dags:/usr/local/airflow/dags
```

Isso garante que qualquer DAG escrita localmente estar√° vis√≠vel dentro do container do Airflow.

Outros pontos relevantes:

* `output/` tamb√©m √© montado, pois √© onde salvamos dados extra√≠dos
* Os logs da execu√ß√£o ficam em `logs/`
* Plugins customizados estariam em `plugins/`, mesmo que n√£o tenhamos usado aqui

---

### üîπ Estrutura de diret√≥rios

```bash
.
‚îú‚îÄ‚îÄ dags/                  # Cont√©m as DAGs e o m√≥dulo utils/
‚îÇ   ‚îú‚îÄ‚îÄ utils/             # Fun√ß√µes auxiliares reutiliz√°veis (como leitura de vari√°veis e logs)
‚îú‚îÄ‚îÄ logs/                  # Logs gerados pela execu√ß√£o do Airflow
‚îú‚îÄ‚îÄ output/                # Arquivos de sa√≠da gerados pelas DAGs
‚îú‚îÄ‚îÄ infra/                 # Cont√©m o docker-compose.yml
‚îú‚îÄ‚îÄ start.sh, stop.sh, reset.sh, logs.sh  # Scripts auxiliares
‚îú‚îÄ‚îÄ Makefile               # Alternativa centralizada para os scripts
```

---

## Scripts de Gerenciamento do Ambiente

Para simplificar a intera√ß√£o com o ambiente Docker do Apache Airflow e otimizar as tarefas do dia a dia, este projeto utiliza uma s√©rie de scripts shell (`.sh`). Eles abstraem comandos complexos do Docker e do Airflow CLI, tornando o processo de inicializa√ß√£o, parada e monitoramento muito mais intuitivo.

**Importante:** Antes de executar qualquer script `.sh` pela primeira vez, certifique-se de que ele tenha permiss√µes de execu√ß√£o. Voc√™ pode conceder essas permiss√µes usando o comando `chmod +x nome_do_script.sh` no terminal.

A seguir, a descri√ß√£o e o modo de uso de cada script:

### 1. `start.sh` - Iniciando o Ambiente Airflow

*   **Fun√ß√£o:** Este script √© o ponto de entrada para levantar todo o seu ambiente Apache Airflow, incluindo os servi√ßos de Webserver, Scheduler, Worker, banco de dados (Postgres) e Redis, todos dentro de cont√™ineres Docker. Ele tamb√©m garante que o banco de metadados do Airflow seja inicializado e um usu√°rio administrador seja criado, se ainda n√£o existirem.
*   **Quando usar:** Sempre que voc√™ precisar iniciar o ambiente Airflow do zero ou reinici√°-lo ap√≥s uma parada.
*   **Como usar:**
    ```bash
    ./start.sh
    ```
*   **Detalhes:** Ele executa os comandos `docker compose up -d` (para iniciar os servi√ßos em segundo plano) e `docker compose exec airflow-webserver airflow db upgrade`, `airflow users create`, entre outros, para configurar o Airflow.

### 2. `stop.sh` - Parando o Ambiente Airflow

*   **Fun√ß√£o:** Este script √© respons√°vel por parar todos os servi√ßos do Airflow que est√£o rodando em cont√™ineres Docker. Ele desliga os cont√™ineres de forma controlada.
*   **Quando usar:** Quando voc√™ terminar de trabalhar no projeto e quiser liberar os recursos do seu computador, ou antes de realizar altera√ß√µes profundas no ambiente.
*   **Como usar:**
    ```bash
    ./stop.sh
    ```
*   **Detalhes:** Ele executa o comando `docker compose down`, que para e remove os cont√™ineres, mas mant√©m os volumes de dados para persist√™ncia.

### 3. `reset.sh` - Resetando o Ambiente Airflow (Com Cuidado!)

*   **Fun√ß√£o:** Este √© um script de "limpeza total". Ele n√£o apenas para os servi√ßos do Airflow, mas tamb√©m remove *todos* os volumes de dados associados ao projeto (incluindo o banco de dados do Airflow). Isso significa que voc√™ perder√° o hist√≥rico de execu√ß√£o de DAGs, logs antigos e configura√ß√µes de usu√°rios.
*   **Quando usar:** **Use com extrema cautela!** √â ideal para quando voc√™ quer come√ßar completamente do zero, como se tivesse acabado de clonar o reposit√≥rio, ou para resolver problemas persistentes com o banco de dados do Airflow. Na maioria dos casos, `stop.sh` e `start.sh` s√£o suficientes.
*   **Como usar:**
    ```bash
    ./reset.sh
    ```
*   **Detalhes:** Ele executa `docker compose down -v --remove-orphans`, garantindo uma limpeza profunda.

### 4. `logs.sh` - Visualizando os Logs do Ambiente

*   **Fun√ß√£o:** Este script permite visualizar os logs de todos os servi√ßos do Airflow rodando em cont√™ineres Docker em tempo real. √â essencial para depura√ß√£o e monitoramento do que est√° acontecendo no seu ambiente.
*   **Quando usar:** Sempre que precisar diagnosticar um problema, verificar se os servi√ßos est√£o iniciando corretamente ou acompanhar a execu√ß√£o de tasks.
*   **Como usar:**
    ```bash
    ./logs.sh
    ```
*   **Detalhes:** Ele executa `docker compose logs -f`, mostrando a sa√≠da de log de todos os cont√™ineres e acompanhando novas linhas (modo "follow"). Pressione `Ctrl+C` para sair da visualiza√ß√£o de logs.

---

### üöÄ Outra Op√ß√£o para Execu√ß√£o (Makefile)

1. Ative o ambiente virtual com `poetry shell`
2. Inicie os containers:

```bash
make start
# ou
bash start.sh
```

3. Acesse o Airflow em `http://localhost:8080` com:

* **Login:** airflow
* **Senha:** airflow

4. Execute a DAG desejada na interface ou aguarde o agendamento

5. Veja os arquivos gerados na pasta `output/`

6. Acompanhe os logs com:

```bash
./logs.sh
```

---